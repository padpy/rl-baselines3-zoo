# Tuned
MountainCarContinuous-v0:
  n_timesteps: 300000
  policy: 'MlpPolicy'
  noise_type: 'ornstein-uhlenbeck'
  noise_std: 0.5

Pendulum-v1:
  n_timesteps: 20000
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

LunarLanderContinuous-v2:
  n_timesteps: !!float 3e5
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

PandaReach-v4:
  n_timesteps: !!float 5e5
  policy: 'MultiInputPolicy'
  gamma: 0.9
  batch_size: 128
  buffer_size: 128
  learning_starts: 128
  tau: 0.08
  noise_type: 'normal'
  noise_std: 0.44101
  gradient_steps: -1
  train_freq: [16, "episode"]
  learning_rate: !!float 0.00017153298579370132
  policy_kwargs: "dict(net_arch=[400, 300])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaReachDense-v4:
  n_timesteps: !!float 5e5
  policy: 'MultiInputPolicy'
  gamma: 0.9
  batch_size: 128
  buffer_size: 128
  learning_starts: 128
  tau: 0.08
  noise_type: 'normal'
  noise_std: 0.44101
  gradient_steps: -1
  train_freq: [16, "episode"]
  learning_rate: !!float 0.00017153298579370132
  policy_kwargs: "dict(net_arch=[400, 300])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaGrasp-v3:
  n_timesteps: !!float 1e6
  policy: 'MultiInputPolicy'
  gamma: 0.9
  batch_size: 128
  buffer_size: 10000
  learning_starts: 128
  tau: 0.08
  noise_type: 'normal'
  noise_std: 0.44101
  gradient_steps: -1
  train_freq: [16, "episode"]
  learning_rate: !!float 0.00017153298579370132
  policy_kwargs: "dict(net_arch=[400, 300])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaGraspDense-v3:
  n_timesteps: !!float 1e6
  policy: 'MultiInputPolicy'
  gamma: 0.9
  batch_size: 128
  buffer_size: 10000
  learning_starts: 128
  tau: 0.08
  noise_type: 'normal'
  noise_std: 0.44101
  gradient_steps: -1
  train_freq: [16, "episode"]
  learning_rate: !!float 0.00017153298579370132
  policy_kwargs: "dict(net_arch=[400, 300])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaPickAndPlace-v3:
  n_timesteps: !!float 1e6
  policy: 'MultiInputPolicy'
  gamma: 0.9
  batch_size: 128
  buffer_size: 10000
  learning_starts: 128
  tau: 0.08
  noise_type: 'normal'
  noise_std: 0.44101
  gradient_steps: -1
  train_freq: [16, "episode"]
  learning_rate: !!float 0.00017153298579370132
  policy_kwargs: "dict(net_arch=[400, 300])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaPickAndPlaceDense-v3:
  n_timesteps: !!float 1e6
  policy: 'MultiInputPolicy'
  gamma: 0.9
  batch_size: 128
  buffer_size: 10000
  learning_starts: 128
  tau: 0.08
  noise_type: 'normal'
  noise_std: 0.44101
  gradient_steps: -1
  train_freq: [16, "episode"]
  learning_rate: !!float 0.00017153298579370132
  policy_kwargs: "dict(net_arch=[400, 300])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

BipedalWalker-v3:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

# To be tuned
BipedalWalkerHardcore-v3:
  n_timesteps: !!float 1e7
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

# Tuned
HalfCheetahBulletEnv-v0: &pybullet-defaults
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: 1
  train_freq: 1
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

# Tuned
AntBulletEnv-v0:
  <<: *pybullet-defaults
  learning_rate: !!float 7e-4
  policy_kwargs: "dict(net_arch=[400, 300])"

# Tuned
HopperBulletEnv-v0:
  <<: *pybullet-defaults
  train_freq: 64
  gradient_steps: 64
  batch_size: 256
  learning_rate: !!float 7e-4

# Tuned
Walker2DBulletEnv-v0:
  <<: *pybullet-defaults
  batch_size: 256
  learning_rate: !!float 7e-4

# TO BE tested
HumanoidBulletEnv-v0:
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

# Tuned
ReacherBulletEnv-v0:
  <<: *pybullet-defaults
  n_timesteps: !!float 3e5


# To be tuned
InvertedDoublePendulumBulletEnv-v0:
  <<: *pybullet-defaults
  n_timesteps: !!float 1e6

# To be tuned
InvertedPendulumSwingupBulletEnv-v0:
  <<: *pybullet-defaults
  n_timesteps: !!float 3e5

# === Mujoco Envs ===

HalfCheetah-v3: &mujoco-defaults
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1

Ant-v3:
  <<: *mujoco-defaults

Hopper-v3:
  <<: *mujoco-defaults

Walker2d-v3:
  <<: *mujoco-defaults

Humanoid-v3:
  <<: *mujoco-defaults
  n_timesteps: !!float 2e6

Swimmer-v3:
  <<: *mujoco-defaults
  gamma: 0.9999
  train_freq: 1
  gradient_steps: 1
