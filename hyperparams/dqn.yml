atari:
  env_wrapper:
    - stable_baselines3.common.atari_wrappers.AtariWrapper
  frame_stack: 4
  policy: 'CnnPolicy'
  n_timesteps: !!float 1e7
  buffer_size: 100000
  learning_rate: !!float 1e-4
  batch_size: 32
  learning_starts: 100000
  target_update_interval: 1000
  train_freq: 4
  gradient_steps: 1
  exploration_fraction: 0.1
  exploration_final_eps: 0.01
  # If True, you need to deactivate handle_timeout_termination
  # in the replay_buffer_kwargs
  optimize_memory_usage: False

# Almost Tuned
CartPole-v1:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  learning_rate: !!float 2.3e-3
  batch_size: 64
  buffer_size: 100000
  learning_starts: 1000
  gamma: 0.99
  target_update_interval: 10
  train_freq: 256
  gradient_steps: 10
  exploration_fraction: 0.16
  exploration_final_eps: 0.04
  policy_kwargs: "dict(net_arch=[256, 256])"

# Not Tuned
PandaReachDenseDiscrete-v4:
  n_timesteps: !!float 5e5
  policy: 'MultiInputPolicy'
  learning_rate: !!float 0.00005
  batch_size: 128
  buffer_size: 10000
  learning_starts: 128
  gamma: 0.99
  target_update_interval: 1000
  train_freq: 32
  gradient_steps: -1
  exploration_fraction: 0.2
  exploration_initial_eps: 0.5
  exploration_final_eps: 0.01
  policy_kwargs: "dict(net_arch=[256, 256])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaReachDiscrete-v4:
  n_timesteps: !!float 5e5
  policy: 'MultiInputPolicy'
  learning_rate: !!float 0.00005
  batch_size: 32
  buffer_size: 10000
  learning_starts: 128
  gamma: 0.99
  target_update_interval: 1000
  train_freq: 8
  gradient_steps: -1
  exploration_fraction: 0.2
  exploration_initial_eps: 0.5
  exploration_final_eps: 0.01
  policy_kwargs: "dict(net_arch=[256, 256])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaGraspDiscrete-v3:
  n_timesteps: !!float 1e6
  policy: 'MultiInputPolicy'
  learning_rate: !!float 0.00005
  batch_size: 128
  buffer_size: 50000
  learning_starts: 128
  gamma: 0.99
  target_update_interval: 1000
  train_freq: 64
  gradient_steps: -1
  exploration_fraction: 0.5
  exploration_initial_eps: 0.5
  exploration_final_eps: 0.01
  policy_kwargs: "dict(net_arch=[256, 256])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaGraspDenseDiscrete-v3:
  n_timesteps: !!float 1e6
  policy: 'MultiInputPolicy'
  learning_rate: !!float 0.00005
  batch_size: 128
  buffer_size: 50000
  learning_starts: 128
  gamma: 0.99
  target_update_interval: 1000
  train_freq: 64
  gradient_steps: -1
  exploration_fraction: 0.5
  exploration_initial_eps: 0.5
  exploration_final_eps: 0.01
  policy_kwargs: "dict(net_arch=[256, 256])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaPickAndPlaceDiscrete-v3:
  n_timesteps: !!float 1e6
  policy: 'MultiInputPolicy'
  learning_rate: !!float 0.00005
  batch_size: 128
  buffer_size: 50000
  learning_starts: 128
  gamma: 0.99
  target_update_interval: 1000
  train_freq: 64
  gradient_steps: -1
  exploration_fraction: 0.5
  exploration_initial_eps: 0.5
  exploration_final_eps: 0.01
  policy_kwargs: "dict(net_arch=[256, 256])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

PandaPickAndPlaceDenseDiscrete-v3:
  n_timesteps: !!float 1e6
  policy: 'MultiInputPolicy'
  learning_rate: !!float 0.00005
  batch_size: 128
  buffer_size: 50000
  learning_starts: 128
  gamma: 0.99
  target_update_interval: 1000
  train_freq: 64
  gradient_steps: -1
  exploration_fraction: 0.5
  exploration_initial_eps: 0.5
  exploration_final_eps: 0.01
  policy_kwargs: "dict(net_arch=[256, 256])"
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    online_sampling=True,
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"

# Tuned
MountainCar-v0:
  n_timesteps: !!float 1.2e5
  policy: 'MlpPolicy'
  learning_rate: !!float 4e-3
  batch_size: 128
  buffer_size: 10000
  learning_starts: 1000
  gamma: 0.98
  target_update_interval: 600
  train_freq: 16
  gradient_steps: 8
  exploration_fraction: 0.2
  exploration_final_eps: 0.07
  policy_kwargs: "dict(net_arch=[256, 256])"

# Tuned
LunarLander-v2:
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  learning_rate: !!float 6.3e-4
  batch_size: 128
  buffer_size: 50000
  learning_starts: 0
  gamma: 0.99
  target_update_interval: 250
  train_freq: 4
  gradient_steps: -1
  exploration_fraction: 0.12
  exploration_final_eps: 0.1
  policy_kwargs: "dict(net_arch=[256, 256])"

# Tuned
Acrobot-v1:
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  learning_rate: !!float 6.3e-4
  batch_size: 128
  buffer_size: 50000
  learning_starts: 0
  gamma: 0.99
  target_update_interval: 250
  train_freq: 4
  gradient_steps: -1
  exploration_fraction: 0.12
  exploration_final_eps: 0.1
  policy_kwargs: "dict(net_arch=[256, 256])"
